
import numpy
import math
from scipy import spatial
def get_bag(n):
    """
    this function reads twits and make co-occurency matrix from them

    :param n:  the minimum number of total occurency word in text to take rhis word into account
    values are good to use:
    for number of twits <200 n = 5
    for number of twits >1000 n = 20
    :return:
    matrix co-occurency , dictt - list of words
    matrix and dictt have the same words order
    """
    y=0
    dictt= []
    first_dict={}  # dict with number of total occuring of word
    ff = open("filtered.csv")
    for line in ff:
        for word in line.split():
            if word not in first_dict:
                first_dict[word] = 0
            first_dict[word] += 1
    ff.close()

    fff = open("to_write1.csv", 'w')    # file to save only often occured words
    ff = open("filtered.csv")

    for line in ff:
        new_line = ""
        i=0
        for word in line.split():
            if first_dict[word] > n:
                new_line += word
                new_line += " "
                i+=1
        if  i > 1:
            new_line = new_line[:-1]+","+"\n"
            fff.write(new_line)
    ff.close()
    fff.close()
    ff = open ("to_write1.csv" ,"r")
    d = {}
    # Getting word and corresponding vector from each line of the model.vec file generated by fasttext
    import codecs  # To open the file in specific mode
    for line in ff:
        words = line.split(",")[0]
        #print(words)
        for w in words.split(" "):
            if w not in dictt:
                dictt.append(w)

    #create empty matrix
    matrix = numpy.zeros((len(dictt),len(dictt)))

    for i in range(0, len(dictt)):
        for ii in range(0, len(dictt)):
            matrix[i][ii]=0


    ff = open ("to_write1.csv" ,"r")
    for line in ff:
        words = line.split(",")[0]
        #print(words)
        for w in words.split(" "):
            for ww in words.split(" "):
                if  w != ww:
                    matrix[dictt.index(w)][dictt.index(ww)]+=1
    ff.close()
    return matrix, dictt
matrix , dictt = get_bag(0)
#print(matrix)
#print(dictt)


n_dictt=[]

from sklearn.decomposition import PCA
from matplotlib import pyplot as plt

pca = PCA()
res = pca.fit_transform(matrix)
new_res = []
for el in res:
    el = list(el)
    new_res.append(el)

lsss=[]

new_d = {}
for i,el in enumerate(dictt):
   new_d[el]= res[i]

def euclidean_distance(x, y):
    """Computes the Euclidean distance between two 1-D arrays.  """""
    distance = math.sqrt(sum([(a - b) ** 2 for a, b in zip(x, y)]))
    return distance

def find_closest_embeddings_pca(word, number):

    result =  sorted(new_d.keys(), key=lambda wordd: euclidean_distance(new_res[dictt.index(wordd)], new_res[word]))
    return    result[:number]

print(find_closest_embeddings_pca(dictt.index("china"))[:20])

def matr_pca_2_comp(A):
    # 1.Take the whole dataset consisting of d+1 dimensions, scale it
    # and ignore the labels such that our new dataset becomes d dimensional.

    A_scaled = StandardScaler().fit_transform(numpy.array(A))

    # Compute the mean for every dimension of the whole dataset.
    A_mean = numpy.mean(A, axis=0)

    # Compute the covariance matrix of the whole dataset.
    A_scaled_tr = A_scaled.T
    covariance_matrix = numpy.cov(A_scaled_tr)
    # Compute eigenvectors and the corresponding eigenvalues.
    eig_vals, eig_vecs = numpy.linalg.eig(covariance_matrix)
    # k eigenvectors with the largest eigenvalues to form a d × k dimensional matrix
    # Use this d × k eigenvector matrix to transform the samples onto the new subspace.
    result = pandas.DataFrame(columns=['PC1', 'PC2'])
    result['PC1'] = A_scaled.dot(eig_vecs.T[0])
    result['PC2'] = A_scaled.dot(eig_vecs.T[1])
    return result

res2 = matr_pca_2_comp(matrix)

#print(res)
for i in range(0, len(res)):
    plt.text(res2["PC1"][i], res2["PC2"][i], dictt[i], fontdict=None)
plt.show()


