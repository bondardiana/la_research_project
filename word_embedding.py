dictt= []
import numpy
first_dict={}
ff = open("to_write.csv")
for line in ff:
    line = line.split(',')[0]
    for word in line.split():
        if word not in first_dict:
            first_dict[word]=0
        first_dict[word] += 1
ff.close()
fff = open ("to_write1.csv", 'w')
ff = open("to_write.csv")

for line in ff:
    line = line.split(',')[0]
    new_line =""
    i=0
    for word in line.split():
        if first_dict[word] > 10:
            new_line += word
            new_line += " "
            i+=1
    if  i > 1:
        new_line = new_line[:-1]+","+"\n"
        fff.write(new_line)

ff.close()
fff.close()

ff = open ("to_write1.csv" ,"r")
d = {}

# Getting word and corresponding vector from each line of the model.vec file generated by fasttext

import codecs  # To open the file in specific mode
import numpy as np

for line in ff:
    words = line.split(",")[0]
    #print(words)
    for w in words.split(" "):
        if w not in dictt:
            dictt.append(w)

import numpy as np
n_dictt=[]

#dictt= n_dictt
matrix = np.zeros((len(dictt),len(dictt)))

for i in range(0, len(dictt)):
    for ii in range(0, len(dictt)):
        matrix[i][ii]=0



ff = open ("to_write1.csv" ,"r")
for line in ff:
    words = line.split(",")[0]
    #print(words)
    for w in words.split(" "):
        for ww in words.split(" "):
            if  w != ww:
                matrix[dictt.index(w)][dictt.index(ww)]+=1


#print(matrix)
#print(dictt)

for i , el in enumerate(matrix[dictt.index('coronavirus')]):
    if el>2:
        pass
        #print(dictt[i], el)


from sklearn.decomposition import PCA
from matplotlib import pyplot as plt

pca = PCA()
res = pca.fit_transform(matrix)


#print(res)
for i in range(0, len(res)):
    plt.text(res[i][0], res[i][1], dictt[i], fontdict=None)
plt.show()

new_res = []
for el in res:
    el = list(el)
    new_res.append(el)

from scipy import spatial

print(spatial.distance.euclidean(res[dictt.index('covid')], res[dictt.index('coronavirus')]) )
lsss=[]

#for i, el in enumerate(new_res):
#    if dictt[i] == 'home':
#        for ii, elel in enumerate(new_res):
#            if i != ii  and spatial.distance.euclidean(el, elel) <= 50:

#                lsss.append((dictt[i], dictt[ii],spatial.distance.euclidean(el, elel)))
#print(sorted(lsss , key = lambda x: x[2]) )

new_d = {}
for i,el in enumerate(dictt):
   new_d[el]= res[i]



def find_closest_embeddings_pca(i):
    return sorted(new_d.keys(), key=lambda word: spatial.distance.euclidean(new_res[dictt.index(word)], new_res[i]))
print(find_closest_embeddings_pca(dictt.index("home"))[:20])